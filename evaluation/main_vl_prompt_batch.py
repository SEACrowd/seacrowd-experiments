"""nusacrowd zero-shot prompt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ru8DyS2ALWfRdkjOPHj-KNjw6Pfa44Nd
"""
import os, sys
import csv
import string

from os.path import exists

import numpy as np
from numpy import argmax, stack
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import classification_report, precision_recall_fscore_support

import torch
import torch.nn.functional as F

from peft import PeftModel
from transformers import AutoModelForVision2Seq, AutoProcessor, set_seed
from transformers.image_utils import load_image

from prompt_utils import get_prompt, get_label_mapping
from data_utils import load_vl_datasets
from seacrowd.sea_datasets.sea_wiki.lang_config import _LANG_CONFIG

import cv2
import urllib

#!pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp
#!pip install transformers
#!pip install sentencepiece

DEBUG=False

csv.field_size_limit(sys.maxsize)

def read_image(impath):
    req = urllib.request.urlopen(impath)
    arr = np.asarray(bytearray(req.read()), dtype=np.uint8)
    img = cv2.imdecode(arr, -1)
    return img


def get_lang(language):
    if language in _LANG_CONFIG:
        return _LANG_CONFIG[language]
    return language


def to_prompt(input, prompt, language, prompt_lang, schema):
    if schema == "imtext":
        prompt = prompt.replace('[LANGUAGE]', get_lang(language))
        image = input['image_paths'][0]
        # prompt = prompt.replace('[IMAGE]', read_image(input['image_paths'][0]))
        # prompt = prompt.replace('[IMAGE]', input['image_paths'][0])
        pass

    elif schema == "imqa":
        # prompt = prompt.replace('[IMAGE]', read_image(input['image_paths'][0]))
        # prompt = prompt.replace('[IMAGE]', input['image_paths'][0])
        # prompt = prompt.replace('[QUESTION]', input['questions'][0])
        pass

    else:
        raise ValueError("Only support `imtext`, `imqa`.")

    return prompt, image


@torch.inference_mode()
def generate_output(model, processor, prompts, images):
    # images = [[read_image(i)] for i in images]
    images = [load_image(i) for i in images]
    inputs = processor(text=prompts, images=images, return_tensors="pt")

    generated_ids = model.generate(**inputs, max_new_tokens=50)
    generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)

    return generated_texts
    

if __name__ == '__main__':
    if len(sys.argv) < 4:
        raise ValueError('main_vl_prompt.py <prompt_lang> <model_path_or_name> <batch_size> <save_every (OPTIONAL)>')

    prompt_lang = sys.argv[1]
    MODEL = sys.argv[2]
    BATCH_SIZE = int(sys.argv[3])
    ADAPTER = ''
    if 'bactrian' in MODEL:
        MODEL, ADAPTER = MODEL.split('---')

    SAVE_EVERY = 10
    if len(sys.argv) == 5:
        SAVE_EVERY = int(sys.argv[4])

    out_dir = './outputs_vl'
    metric_dir = './metrics_vl'
    os.makedirs(out_dir, exist_ok=True) 
    os.makedirs(metric_dir, exist_ok=True) 

    # Load Prompt
    TASK_TYPE_TO_PROMPT = get_prompt(prompt_lang)

    # Load Dataset
    print('Load VL Datasets...')
    vl_datasets = load_vl_datasets()

    print(f'Loaded {len(vl_datasets)} VL datasets')
    for i, dset_subset in enumerate(vl_datasets.keys()):
        print(f'{i} {dset_subset}')

    # Set seed before initializing model.
    set_seed(42)

    # Load Model
    model = AutoModelForVision2Seq.from_pretrained(MODEL, device_map="auto", trust_remote_code=True)
    processor = AutoProcessor.from_pretrained(MODEL)
        
    model.eval()
    with torch.no_grad():

        metrics = []
        for i, dset_subset in enumerate(vl_datasets.keys()):

            print(f'({i}/{len(vl_datasets.keys())}) {dset_subset}')

            schema = dset_subset.split("_")[-1]
            vl_dset, language, task_type = vl_datasets[dset_subset]
            if task_type.value not in TASK_TYPE_TO_PROMPT:
                print(f'SKIPPING {dset_subset}')
                continue

            # Retrieve metadata
            split = 'test'
            if 'test' in vl_dset.keys():
                test_dset = vl_dset['test']
            else:
                test_dset = vl_dset['train']
                split = 'train'
            print(f'Processing {dset_subset}')
                            
            for prompt_id, prompt_template in enumerate(TASK_TYPE_TO_PROMPT[task_type.value]):
                inputs, preds = [], []
                
                # Check saved data
                if exists(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv'):
                    print("Output exist, use partial log instead")
                    with open(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv') as csvfile:
                        reader = csv.DictReader(csvfile)
                        for row in reader:
                            inputs.append(row["Input"])
                            preds.append(row["Pred"])
                    print(f"Skipping until {len(preds)}")

                # sample prompt
                print("= SAMPLE PROMPT =")
                print(to_prompt(test_dset[0], prompt_template, language, prompt_lang, schema))
                print("\n")

                # zero-shot inference
                prompts, images = [], []
                count = 0
                with torch.inference_mode():
                    for e, sample in tqdm(enumerate(test_dset), total=len(test_dset)):
                        if e < len(preds):
                            continue

                        prompt_text, image = to_prompt(sample, prompt_template, language, prompt_lang, schema)
                        prompts.append(prompt_text)
                        images.append(image)

                        # Batch Inference
                        if len(prompts) == BATCH_SIZE:
                            outputs = generate_output(model, processor, prompts, images)
                            for (prompt_text, image, output_text) in zip(prompts, images, outputs):
                                inputs.append(prompt_text)
                                preds.append(output_text)
                            prompts, images = [], []
                            count += 1
                            
                        if count == SAVE_EVERY:
                            # partial saving
                            inference_df = pd.DataFrame(list(zip(inputs, preds)), columns =["Input", 'Pred'])
                            inference_df.to_csv(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv', index=False)
                            count = 0
                            
                    if len(prompts) > 0:
                        outputs = generate_output(model, processor, prompts, images)
                        for (prompt_text, output_text) in zip(prompts, outputs):
                            inputs.append(prompt_text)
                            preds.append(output_text)
                        prompts = []

                # partial saving
                inference_df = pd.DataFrame(list(zip(inputs, preds)), columns =["Input", 'Pred'])
                inference_df.to_csv(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv', index=False)

                # cls_report = classification_report(golds, preds, output_dict=True)
                # micro_f1, micro_prec, micro_rec, _ = precision_recall_fscore_support(golds, preds, average='micro')
                # print(dset_subset)
                # print('accuracy', cls_report['accuracy'])
                # print('f1 micro', micro_f1)
                # print('f1 macro', cls_report['macro avg']['f1-score'])
                # print('f1 weighted', cls_report['weighted avg']['f1-score'])
                # print("===\n\n")       

                # metrics.append({
                #     'dataset': dset_subset,
                #     'prompt_id': prompt_id,
                #     'prompt_lang': prompt_lang,
                #     'accuracy': cls_report['accuracy'], 
                #     'micro_prec': micro_prec,
                #     'micro_rec': micro_rec,
                #     'micro_f1_score': micro_f1,
                #     'macro_prec': cls_report['macro avg']['precision'],
                #     'macro_rec': cls_report['macro avg']['recall'],
                #     'macro_f1_score': cls_report['macro avg']['f1-score'],
                #     'weighted_prec': cls_report['weighted avg']['precision'],
                #     'weighted_rec': cls_report['weighted avg']['recall'],
                #     'weighted_f1_score': cls_report['weighted avg']['f1-score'],
                # })

    # pd.DataFrame(metrics).reset_index().to_csv(f'{metric_dir}/vl_results_{prompt_lang}_{MODEL.split("/")[-1]}.csv', index=False)
