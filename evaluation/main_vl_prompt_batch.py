"""nusacrowd zero-shot prompt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ru8DyS2ALWfRdkjOPHj-KNjw6Pfa44Nd
"""
import os, sys
import csv
import string

from os.path import exists

import numpy as np
from numpy import argmax, stack
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import classification_report, precision_recall_fscore_support

import torch
import torch.nn.functional as F

from peft import PeftModel
from transformers import AutoModelForVision2Seq, PaliGemmaForConditionalGeneration, AutoProcessor, set_seed
from transformers.image_utils import load_image

from prompt_utils import get_prompt, get_label_mapping
from data_utils import load_vl_datasets
from seacrowd.sea_datasets.sea_wiki.lang_config import _LANG_CONFIG

import cv2
import urllib
import requests
import PIL
from PIL import Image
from metrics_utils import generation_metrics_fn

#!pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp
#!pip install transformers
#!pip install sentencepiece

DEBUG=False

csv.field_size_limit(sys.maxsize)


def get_lang(language):
    lang_dict = _LANG_CONFIG
    lang_dict['ceb'] = 'Cebuano'
    lang_dict['fil'] = 'Filipino'
    lang_dict['ind'] = lang_dict['id'] = 'Indonesian'
    lang_dict['tha'] = lang_dict['th'] = 'Thai'
    lang_dict['vie'] = lang_dict['vi'] = 'Vietnamese'
    lang_dict['war'] = 'Waray'

    if language in _LANG_CONFIG:
        return _LANG_CONFIG[language]
    return language


def to_prompt(input, prompt, language, prompt_lang, schema):
    if schema == "imtext":
        prompt = prompt.replace('[LANGUAGE]', get_lang(language))
        image = input['image_paths'][0]
        # prompt = prompt.replace('[IMAGE]', read_image(input['image_paths'][0]))
        # prompt = prompt.replace('[IMAGE]', input['image_paths'][0])
        pass

    elif schema == "imqa":
        # prompt = prompt.replace('[IMAGE]', read_image(input['image_paths'][0]))
        # prompt = prompt.replace('[IMAGE]', input['image_paths'][0])
        # prompt = prompt.replace('[QUESTION]', input['questions'][0])
        pass

    else:
        raise ValueError("Only support `imtext`, `imqa`.")

    return prompt, image


@torch.inference_mode()
def generate_output(model, processor, prompts, images):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive'
    }
    if images[0][0] == '/':
        # Local path
        images = [Image.open(i) for i in images]
    else:
        images = [Image.open(requests.get(i, headers=headers, stream=True).raw) for i in images]
    if type(model) == PaliGemmaForConditionalGeneration:
        # Some models only support batch size 1
        assert len(prompts)==1 and len(images)==1
        image_0 = np.array(images[0])
        if len(image_0.shape) == 2:
            images[0] = cv2.cvtColor(image_0, cv2.COLOR_GRAY2RGB)
        elif len(image_0.shape) == 3 and image_0.shape[2] == 4:
            images[0] = Image.fromarray(image_0[:,:,:3])
        inputs = processor(text=prompts[0], images=images[0], return_tensors="pt").to("cuda")
    else:
        inputs = processor(text=prompts, images=images, return_tensors="pt").to("cuda")

    with torch.inference_mode():
        generated_ids = model.generate(**inputs, max_new_tokens=50, do_sample=False)
        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)

    # Remove prefix if present
    for i in range(len(prompts)):
        input_len = len(prompts[i].split("<image>")[0].strip())
        if generated_texts[i][:input_len].strip() == prompts[i][:input_len].strip():
            generated_texts[i] = generated_texts[i][input_len:]

    generated_texts = [i.strip() for i in generated_texts]
    return generated_texts
    

if __name__ == '__main__':
    if len(sys.argv) < 4:
        raise ValueError('main_vl_prompt.py <prompt_lang> <model_path_or_name> <batch_size> <save_every (OPTIONAL)>')

    prompt_lang = sys.argv[1]
    MODEL = sys.argv[2]
    BATCH_SIZE = int(sys.argv[3])
    ADAPTER = ''
    if 'bactrian' in MODEL:
        MODEL, ADAPTER = MODEL.split('---')

    SAVE_EVERY = 10
    if len(sys.argv) == 5:
        SAVE_EVERY = int(sys.argv[4])

    out_dir = './outputs_vl'
    metric_dir = './metrics_vl'
    os.makedirs(out_dir, exist_ok=True) 
    os.makedirs(metric_dir, exist_ok=True) 

    # Load Prompt
    TASK_TYPE_TO_PROMPT = get_prompt(prompt_lang)

    # Load Dataset
    print('Load VL Datasets...')
    vl_datasets = load_vl_datasets()

    print(f'Loaded {len(vl_datasets)} VL datasets')
    for i, dset_subset in enumerate(vl_datasets.keys()):
        print(f'{i} {dset_subset}')

    # Set seed before initializing model.
    set_seed(42)

    # Load Model & Processor
    model = AutoModelForVision2Seq.from_pretrained(MODEL, device_map="auto", trust_remote_code=True)
    processor = AutoProcessor.from_pretrained(MODEL)
        
    model.eval()
    with torch.no_grad():

        metrics = {'dataset': []}
        for i, dset_subset in enumerate(vl_datasets.keys()):

            print(f'({i}/{len(vl_datasets.keys())}) {dset_subset}')

            schema = dset_subset.split("_")[-1]
            vl_dset, language, task_type = vl_datasets[dset_subset]
            if task_type.value not in TASK_TYPE_TO_PROMPT:
                print(f'SKIPPING {dset_subset}')
                continue

            # Retrieve metadata
            split = 'test'
            if 'test' in vl_dset.keys():
                test_dset = vl_dset['test']
            else:
                test_dset = vl_dset['train']
                split = 'train'
            print(f'Processing {dset_subset}')

            for prompt_id, prompt_template in enumerate(TASK_TYPE_TO_PROMPT[task_type.value]):
                inputs, preds, golds = [], [], []
                
                # Check saved data
                if exists(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv'):
                    print("Output exist, use partial log instead")
                    with open(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv') as csvfile:
                        reader = csv.DictReader(csvfile)
                        for row in reader:
                            inputs.append(row["Input"])
                            preds.append(row["Pred"])
                            golds.append(row["Gold"])
                    print(f"Skipping until {len(preds)}")

                # sample prompt
                print("= SAMPLE PROMPT =")
                print(to_prompt(test_dset[0], prompt_template, language, prompt_lang, schema))
                print("\n")

                # zero-shot inference
                prompts, images, batch_golds = [], [], []
                count = 0
                with torch.inference_mode():
                    for e, sample in tqdm(enumerate(test_dset), total=len(test_dset)):
                        if e < len(preds):
                            continue

                        prompt_text, image = to_prompt(sample, prompt_template, language, prompt_lang, schema)
                        prompts.append(prompt_text)
                        images.append(image)
                        batch_golds.append(sample['texts'])

                        # Batch Inference
                        if len(prompts) == BATCH_SIZE:
                            try:
                                outputs = generate_output(model, processor, prompts, images)
                            except PIL.UnidentifiedImageError:
                                print(f"ERROR! Cannot read image file: {images}. If bsz >1, consider rerunning with bsz=1")
                                outputs = ""
                            for (prompt_text, image, output_text, gold) in zip(prompts, images, outputs, batch_golds):
                                inputs.append(prompt_text)
                                preds.append(output_text)
                                golds.append(gold)
                            prompts, images, batch_golds = [], [], []
                            count += 1
                            
                        if count == SAVE_EVERY:
                            # partial saving
                            inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
                            inference_df.to_csv(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv', index=False)
                            count = 0
                            
                    # Predict the rest
                    if len(prompts) > 0:
                        outputs = generate_output(model, processor, prompts, images)
                        for (prompt_text, image, output_text, gold) in zip(prompts, images, outputs, batch_golds):
                            inputs.append(prompt_text)
                            preds.append(output_text)
                            golds.append(gold)
                        prompts, images, batch_golds = [], [], []

                # partial saving
                inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
                inference_df.to_csv(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv', index=False)

                eval_metric = generation_metrics_fn(preds, golds)

                print(f'== {dset_subset} == ')
                for k, v in eval_metric.items():
                    print(k, v)            
                print("===\n\n")
                eval_metric['prompt_id'] = prompt_id

                metrics['dataset'].append(dset_subset)
                for k in eval_metric:
                    if k not in metrics:
                        metrics[k] = []
                    metrics[k].append(eval_metric[k])
            pd.DataFrame(metrics).reset_index().to_csv(f'{metric_dir}/vl_results_{prompt_lang}_{MODEL.split("/")[-1]}.csv', index=False)
