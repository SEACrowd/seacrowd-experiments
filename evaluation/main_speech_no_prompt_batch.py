"""nusacrowd zero-shot prompt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ru8DyS2ALWfRdkjOPHj-KNjw6Pfa44Nd
"""
import os, sys
import csv, json
import string, re

from os.path import exists

from numpy import argmax, stack
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import classification_report, precision_recall_fscore_support

import torch
import torch.nn.functional as F
import torchaudio
import jiwer

from peft import PeftModel
from transformers import (
    set_seed,
    Wav2Vec2Processor,
    Wav2Vec2CTCTokenizer,
    Wav2Vec2FeatureExtractor,
    Wav2Vec2ForCTC,
    Wav2Vec2Config,
    Trainer,
    TrainingArguments,
    HfArgumentParser,
    EarlyStoppingCallback
)

from prompt_utils import get_prompt, get_label_mapping
from data_utils import load_speech_datasets

# !pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp
# !pip install transformers
# !pip install sentencepiece

DEBUG = False

csv.field_size_limit(sys.maxsize)
device = "cuda"

#####
# Common Functions
#####
CHARS_TO_IGNORE = [",", "?", "¿", ".", "!", "¡", ";", "；", ":", '""', "%", '"', "�", "ʿ", "·", "჻", "~", "՞",
                   "؟", "،", "।", "॥", "«", "»", "„", "“", "”", "「", "」", "‘", "’", "《", "》", "(", ")",
                   "{", "}", "=", "`", "_", "+", "<", ">", "…", "–", "°", "´", "ʾ", "‹", "›", "©", "®", "—", "→", "。",
                   "、", "﹂", "﹁", "‧", "～", "﹏", "，", "｛", "｝", "（", "）", "［", "］", "【", "】", "‥", "〽",
                   "『", "』", "〝", "〟", "⟨", "⟩", "〜", "：", "！", "？", "♪", "؛", "/", "\\", "º", "−", "^", "ʻ", "ˆ"]

chars_to_ignore_re = f"[{re.escape(''.join(CHARS_TO_IGNORE))}]"

DEFAULT_SAMPLING_RATE = 16000


if __name__ == '__main__':
    if len(sys.argv) < 3:
        raise ValueError('main_speech_no_prompt_batch.py <model_path_or_name> <batch_size> <save_every (OPTIONAL)>')

    MODEL = sys.argv[1]
    BATCH_SIZE = int(sys.argv[2])
    ADAPTER = ''

    SAVE_EVERY = 10
    if len(sys.argv) == 4:
        SAVE_EVERY = int(sys.argv[3])

    out_dir = './outputs_speech_{}'.format(MODEL.split('/')[-1])
    metric_dir = './metrics_speech_{}'.format(MODEL.split('/')[-1])
    os.makedirs(out_dir, exist_ok=True)
    os.makedirs(metric_dir, exist_ok=True)

    def remove_special_characters(batch):
        batch["text"] = re.sub(chars_to_ignore_re, '', batch["text"]).lower().replace("’", "'")
        return batch

    def extract_all_chars(batch):
        all_text = " ".join([text.lower() for text in batch["text"]])
        vocab = list(set(all_text))
        return {"vocab": [vocab], "all_text": [all_text]}

    def _get_pretrained_special_tokens(tokenizer=None):
        special_tokens = {}
        if tokenizer is None:
            special_tokens["bos_token"] = ("<s>", None)
            special_tokens["eos_token"] = ("</s>", None)
            special_tokens["unk_token"] = ("[UNK]", None)
            special_tokens["pad_token"] = ("<pad>", None)
            special_tokens["word_delimiter_token"] = ("|", None)
            special_tokens["do_lower_case"] = False
        else:
            special_tokens["bos_token"] = (tokenizer.bos_token, tokenizer.bos_token_id)
            special_tokens["eos_token"] = (tokenizer.eos_token, tokenizer.eos_token_id)
            special_tokens["unk_token"] = (tokenizer.unk_token, tokenizer.unk_token_id)
            special_tokens["pad_token"] = (tokenizer.pad_token, tokenizer.pad_token_id)
            special_tokens["word_delimiter_token"] = (tokenizer.word_delimiter_token, tokenizer.word_delimiter_token_id)
            special_tokens["do_lower_case"] = tokenizer.do_lower_case
        return special_tokens

    def _assign_id_to_special_tokens(special_tokens, vocab_dict):
        def __get_key_by_value(dict, value):
            return list(dict.keys())[list(dict.values()).index(value)]

        bos_token = special_tokens["bos_token"][0]
        if bos_token not in vocab_dict:
            bos_token_id = special_tokens["bos_token"][1]
            if bos_token_id is None:
                bos_token_id = 1  # common token id for bos in config.json
            vocab_dict[__get_key_by_value(vocab_dict, bos_token_id)] = len(vocab_dict)
            vocab_dict[bos_token] = bos_token_id

        eos_token = special_tokens["eos_token"][0]
        if eos_token not in vocab_dict:
            eos_token_id = special_tokens["eos_token"][1]
            if eos_token_id is None:
                eos_token_id = 2  # common token id for eos in config.json
            vocab_dict[__get_key_by_value(vocab_dict, eos_token_id)] = len(vocab_dict)
            vocab_dict[eos_token] = eos_token_id

        pad_token = special_tokens["pad_token"][0]
        if pad_token not in vocab_dict:
            pad_token_id = special_tokens["pad_token"][1]
            if pad_token_id is None:
                pad_token_id = 0  # common token id for pad in config.json
            vocab_dict[__get_key_by_value(vocab_dict, pad_token_id)] = len(vocab_dict)
            vocab_dict[pad_token] = pad_token_id

        unk_token = special_tokens["unk_token"][0]
        if unk_token not in vocab_dict:
            unk_token_id = special_tokens["unk_token"][1]
            if unk_token_id is None:
                unk_token_id = 3  # common token id for unk, following jonatangrosman's setting
            vocab_dict[__get_key_by_value(vocab_dict, unk_token_id)] = len(vocab_dict)
            vocab_dict[unk_token] = unk_token_id

        word_delimiter_token = special_tokens["word_delimiter_token"][0]
        if word_delimiter_token not in vocab_dict:
            word_delimiter_token_id = special_tokens["word_delimiter_token"][1]
            if word_delimiter_token_id is None:
                word_delimiter_token_id = 4  # common token id for word delimiter, following jonatangrosman's setting
            vocab_dict[__get_key_by_value(vocab_dict, word_delimiter_token_id)] = len(vocab_dict)
            vocab_dict[word_delimiter_token] = word_delimiter_token_id

        return vocab_dict

    # Define compute metric function
    def compute_metrics(pred):
        pred_strs = pred["predicted"]
        label_strs = pred["target"]
        if len(label_strs) == 0:
            return 0, 0
        else:
            wer = jiwer.wer(pred_strs, label_strs)
            mer = jiwer.mer(pred_strs, label_strs)
            cer = jiwer.mer(pred_strs, label_strs)

        metrics = {
            "wer": wer, "mer": mer, "cer": cer
        }

        return metrics

    def map_to_array(batch):
        speech, sr = torchaudio.load(batch["path"])
        if sr != DEFAULT_SAMPLING_RATE:
            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=DEFAULT_SAMPLING_RATE)
            batch["speech"] = resampler.forward(speech.squeeze(0)).numpy()
            batch["sampling_rate"] = DEFAULT_SAMPLING_RATE
        else:
            batch["speech"] = speech
        batch["sentence"] = batch["text"]
        return batch

    def map_to_pred(batch):
        features = processor(batch["speech"], sampling_rate=DEFAULT_SAMPLING_RATE, padding=True, return_tensors="pt")
        input_values = features.input_values.cuda()
        attention_mask = features.attention_mask.cuda()
        with torch.no_grad():
            logits = model(input_values, attention_mask=attention_mask).logits
        pred_ids = torch.argmax(logits, dim=-1)
        batch["predicted"] = processor.batch_decode(pred_ids)
        batch["target"] = batch["sentence"]
        return batch

    # MAIN FLOW STARTS HERE
    # Set seed before initializing model.
    set_seed(42)

    # Build vocabulary
    print('Build vocabulary...')
    vocab_list = []

    # Load Dataset
    print('Load Speech Datasets...')
    speech_datasets = load_speech_datasets()

    print(f'Loaded {len(speech_datasets)} Speech datasets')

    try:
        pretrained_tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(MODEL)
        special_tokens = _get_pretrained_special_tokens(pretrained_tokenizer)
        pretrained_vocab = list(
            map(lambda x: x[0], sorted(pretrained_tokenizer.get_vocab().items(), key=lambda x: x[1])))
    except:
        special_tokens = _get_pretrained_special_tokens()
        pretrained_vocab = []

    print("Vocab length (initial):", len(pretrained_vocab))

    new_vocab_list = []
    for i, dset_subset in enumerate(speech_datasets.keys()):
        print(f'{i} {dset_subset}')
        speech_datasets[dset_subset] = (
            speech_datasets[dset_subset][0].map(remove_special_characters),
            speech_datasets[dset_subset][1]
        )

    if os.path.exists("{}/new_vocab.json".format(out_dir)):
        with open("{}/new_vocab.json".format(out_dir), "r") as new_vocab_file:
            new_vocab_list = json.load(new_vocab_file)

    for i, dset_subset in enumerate(speech_datasets.keys()):
        vocabs = speech_datasets[dset_subset][0].map(
            extract_all_chars,
            batched=True,
            batch_size=-1,
            keep_in_memory=True,
            remove_columns=speech_datasets[dset_subset][0].column_names["train"]
        )

        vocab_list = list(set(vocabs["train"]["vocab"][0]))
        if "validation" in vocabs:
            vocab_list = list(set(vocab_list) | set(vocabs["validation"]["vocab"][0]))
        if "test" in vocabs:
            vocab_list = list(set(vocab_list) | set(vocabs["test"]["vocab"][0]))

    all_vocab = pretrained_vocab + list(dict.fromkeys(new_vocab_list))
    vocab_dict = {v: k for k, v in enumerate(all_vocab)}

    vocab_dict = _assign_id_to_special_tokens(special_tokens, vocab_dict)
    print("len vocab dict", len(vocab_dict))

    with open("{}/all_vocab.json".format(out_dir), "w") as vocab_file:
        json.dump(vocab_dict, vocab_file)

    tokenizer = Wav2Vec2CTCTokenizer(
        "{}/all_vocab.json".format(out_dir),
        bos_token=special_tokens["bos_token"][0],
        eos_token=special_tokens["eos_token"][0],
        pad_token=special_tokens["pad_token"][0],
        unk_token=special_tokens["unk_token"][0],
        word_delimiter_token=special_tokens["word_delimiter_token"][0],
        do_lower_case=special_tokens["do_lower_case"],
        padding=True
    )
    print("Vocab size (final):", tokenizer.vocab_size)

    # Load Model
    print('Load Wav2Vec2 model and processor...')
    config = Wav2Vec2Config.from_pretrained(MODEL)
    config.update({
        "mask_time_prob": 0,
        "mask_time_length": 0,
        "mask_feature_prob": 0,
        "mask_feature_length": 0,
        "gradient_checkpointing": True,
    })\

    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL)
    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
    model = Wav2Vec2ForCTC.from_pretrained(MODEL, config=config).cuda()

    for i, dset_subset in enumerate(speech_datasets.keys()):
        speech_dataset, task = speech_datasets[dset_subset]
        print(dset_subset)
        # Evaluation Phase (Validation)
        if "validation" in speech_dataset.keys():
            print("*** Valid Phase ***")
            ds = speech_dataset["validation"].map(map_to_array)
            print(ds)
            result = ds.map(map_to_pred, batched=True, batch_size=BATCH_SIZE, remove_columns=list(ds.features.keys()))
            metrics = compute_metrics(result)
            pd.DataFrame(metrics).reset_index().to_csv(
                f'{metric_dir}/speech_results_{dset_subset}_validation_{MODEL.split("/")[-1]}.csv',
                index=False
            )

        # Evaluation Phase (Test)
        if "test" in speech_dataset.keys():
            print("*** Test Phase ***")
            ds = speech_dataset["test"].map(map_to_array)
            result = ds.map(map_to_pred, batched=True, batch_size=BATCH_SIZE, remove_columns=list(ds.features.keys()))
            metrics = compute_metrics(result)
            pd.DataFrame(metrics).reset_index().to_csv(
                f'{metric_dir}/speech_results_{dset_subset}_test_{MODEL.split("/")[-1]}.csv',
                index=False
            )
