"""nusacrowd zero-shot prompt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ru8DyS2ALWfRdkjOPHj-KNjw6Pfa44Nd
"""
import os, sys
import csv
import string

from os.path import exists

from numpy import argmax, stack
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import classification_report, precision_recall_fscore_support

import torch
import torch.nn.functional as F

from peft import PeftModel
from transformers import set_seed
from retry import retry

from prompt_utils import get_prompt, get_label_mapping
from data_utils import load_nlu_datasets

import cohere

from dotenv import load_dotenv
import os
from openai import AzureOpenAI, BadRequestError

#!pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp
#!pip install transformers
#!pip install sentencepiece

DEBUG=False

csv.field_size_limit(sys.maxsize)

def get_api_client(model):
    if "cohere" in model:
        client = cohere.Client(
            api_key=os.getenv("COHERE_PROD_API_KEY"),
        )
    elif "openai" in model:
        # Load secrets
        load_dotenv(dotenv_path=os.getenv("OPENAI_ENV_PATH"))
        client = AzureOpenAI(
            azure_endpoint = os.getenv("ENDPOINT"),
            api_key=os.getenv("API-KEY"),
            api_version="2024-02-01",
        )
    else:
        raise ValueError("Only support `cohere` and `openai` models.")  
    return client

# They sometimes timeout
@retry(Exception, tries=5, delay=10)
def get_response(
        client, model, prompt, temperature=0, max_output_tokens=30,
        system_message="Only answer with the label of choice."):
    if "cohere" in model:
        try:
            response = client.chat(
                model=model.split("/")[-1],
                message=prompt,
                preamble=system_message,
                temperature=temperature, # turn off randomness
                max_tokens=max_output_tokens, # keep it low because we only need the label choice for NLU
                seed=SEED,
            ).text
        except cohere.core.api_error.ApiError as e:
            response = "<BAD_REQUEST_ERROR>"
    elif "openai" in model:
        try:
            response = client.chat.completions.create(
                model=os.getenv("DEPLOYMENT-NAME"), # model = "deployment_name".
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_output_tokens,
                seed=SEED,
            )
            response = response.choices[0].message.content
        except BadRequestError as e:
            response = "<BAD_REQUEST_ERROR>"
    else:
        raise ValueError("Only support `cohere` and `openai` models.")
    return response

def to_prompt(input, prompt, labels, prompt_lang, schema):
    if schema == "text" or schema == "pairs":
        # single label
        if 'text' in input:
            prompt = prompt.replace('[INPUT]', input['text'])
        else:
            prompt = prompt.replace('[INPUT_A]', input['text_1'])
            prompt = prompt.replace('[INPUT_B]', input['text_2'])

        # replace [OPTIONS] to A, B, or C
        if "[OPTIONS]" in prompt:
            new_labels = [f'{l}' for l in labels]
            new_labels[-1] = ("or " if 'eng' in prompt_lang else  "atau ") + new_labels[-1] 
            if len(new_labels) > 2:
                prompt = prompt.replace('[OPTIONS]', ', '.join(new_labels))
            else:
                prompt = prompt.replace('[OPTIONS]', ' '.join(new_labels))
    elif schema == "qa":
        if "[CONTEXT]" in prompt:
            context = "" if input['context'] is None else input['context']
            prompt = prompt.replace('[CONTEXT]', context)
        prompt = prompt.replace('[QUESTION]', input['question'])

        choices = "" 
        for i, choice in enumerate(input['choices']):
            if i > 0:
                choices += "\n"
            choices += f"{string.ascii_lowercase[i]}. {choice}"
        prompt = prompt.replace('[ANSWER_CHOICES]', choices)
    else:
        raise ValueError("Only support `text`, `pairs`, and `qa` schemas.")

    return prompt

@torch.inference_mode()
def predict_classification(client, model, prompts, label_names):
    hyps = []
    for prompt in prompts:
        response = get_response(client, model, prompt)
        response = response.strip().lower() if response is not None else ""
        is_found = False
        for i, label_name in enumerate(label_names):
            if not is_found:
                if response == label_name or response.startswith(label_name):
                    hyps.append(i)
                    is_found = True
        if not is_found:
            hyps.append(-1)
    return hyps

if __name__ == '__main__':
    if len(sys.argv) < 4:
        raise ValueError('main_nlu_prompt.py <prompt_lang> <model_path_or_name> <batch_size> <save_every (OPTIONAL)>')

    prompt_lang = sys.argv[1]
    MODEL = sys.argv[2]
    BATCH_SIZE = int(sys.argv[3])
    SEED = 42

    SAVE_EVERY = 1
    if len(sys.argv) == 5:
        SAVE_EVERY = int(sys.argv[4])

    out_dir = './outputs_nlu'
    metric_dir = './metrics_nlu'
    os.makedirs(out_dir, exist_ok=True) 
    os.makedirs(metric_dir, exist_ok=True) 

    # Load Prompt (ONLY ONE PROMPT FOR EACH TASK TYPE)
    TASK_TYPE_TO_PROMPT = get_prompt(prompt_lang, return_only_one=True)

    # Load Dataset
    print('Load NLU Datasets...')
    nlu_datasets = load_nlu_datasets()

    print(f'Loaded {len(nlu_datasets)} NLU datasets')
    for i, dset_subset in enumerate(nlu_datasets.keys()):
        print(f'{i} {dset_subset}')

    # Set seed before initializing model.
    set_seed(SEED)

    # Load Client
    client = get_api_client(MODEL)

    # Begin evaluation
    metrics = []
    labels = []
    for i, dset_subset in enumerate(nlu_datasets.keys()):

        print(f'=====({i}/{len(nlu_datasets.keys())}) {dset_subset} =====')

        schema = dset_subset.split("_")[-1]
        nlu_dset, task_type = nlu_datasets[dset_subset]
        if task_type.value not in TASK_TYPE_TO_PROMPT:
            print(f'SKIPPING {dset_subset}')
            continue

        # Retrieve metadata
        split = 'test'
        if 'test' in nlu_dset.keys():
            test_dset = nlu_dset['test']
        else:
            test_dset = nlu_dset['train']
            split = 'train'
        # test_dset = test_dset.shard(1000000, 0) # UNCOMMENT FOR TESTING
        print(f'Processing {dset_subset}')

        # Add `label` based on `answer` for QA
        if schema == 'qa':
            correct_answer_indices = []
            exclude_idx = []
            for i in range(len(test_dset)):
                if isinstance(test_dset[i]['answer'], list):
                    try:
                        correct_answer_indices += [test_dset[i]['choices'].index(test_dset[i]['answer'][0])]
                    except:
                        exclude_idx.append(i)
                else:
                    correct_answer_indices += [test_dset[i]['choices'].index(test_dset[i]['answer'])]
            test_dset = test_dset.select(
                (
                    i for i in range(len(test_dset)) 
                    if i not in set(exclude_idx)
                )
            )
            test_dset = test_dset.add_column("label", correct_answer_indices)

        # Retrieve & preprocess labels
        try:
            label_names = test_dset.features['label'].names
        except:
            label_names = list(set(test_dset['label']))
            
        # normalize some labels for more natural prompt
        label_mapping = get_label_mapping(dset_subset, prompt_lang)
        label_names = list(map(lambda x: label_mapping[x], label_mapping))

        label_to_id_dict = { l : i for i, l in enumerate(label_names)}
        
        for prompt_id, prompt_template in enumerate(TASK_TYPE_TO_PROMPT[task_type.value]):
            inputs, preds, golds = [], [], []
            
            # Check saved data
            if exists(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv'):
                print("Output exist, use partial log instead")
                with open(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv') as csvfile:
                    reader = csv.DictReader(csvfile)
                    for row in reader:
                        inputs.append(row["Input"])
                        preds.append(row["Pred"])
                        golds.append(row["Gold"])
                print(f"Skipping until {len(preds)}")

            # sample prompt
            print("= LABEL NAME =")
            print(label_names)
            print("= SAMPLE PROMPT =")
            
            print(to_prompt(test_dset[0], prompt_template, label_names, prompt_lang, schema))
            print("\n")

            # zero-shot inference
            prompts, labels = [], []
            count = 0
            with torch.inference_mode():
                for e, sample in tqdm(enumerate(test_dset), total=len(test_dset)):
                    if e < len(preds):
                        continue

                    prompt_text = to_prompt(sample, prompt_template, label_names, prompt_lang, schema)
                    prompts.append(prompt_text)
                    labels.append(label_to_id_dict[sample['label']] if type(sample['label']) == str else sample['label'])

                    # Batch Inference
                    if len(prompts) == BATCH_SIZE:
                        hyps = predict_classification(client, MODEL, prompts, label_names)
                        for (prompt_text, hyp, label) in zip(prompts, hyps, labels):
                            inputs.append(prompt_text)
                            preds.append(hyp)
                            golds.append(label)
                        prompts, labels = [], []
                        count += 1
                        
                    if count == SAVE_EVERY:
                        # partial saving
                        inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
                        inference_df.to_csv(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv', index=False)
                        count = 0
                        
                if len(prompts) > 0:
                    hyps = predict_classification(client, MODEL, prompts, label_names)
                    for (prompt_text, hyp, label) in zip(prompts, hyps, labels):
                        inputs.append(prompt_text)
                        preds.append(hyp)
                        golds.append(label)
                    prompts, labels = [], []

            # partial saving
            inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
            inference_df.to_csv(f'{out_dir}/{dset_subset}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv', index=False)

            cls_report = classification_report(golds, preds, output_dict=True)
            micro_f1, micro_prec, micro_rec, _ = precision_recall_fscore_support(golds, preds, average='micro')
            print(dset_subset)
            print('accuracy', cls_report['accuracy'])
            print('f1 micro', micro_f1)
            print('f1 macro', cls_report['macro avg']['f1-score'])
            print('f1 weighted', cls_report['weighted avg']['f1-score'])
            print("===\n\n")       

            metrics.append({
                'dataset': dset_subset,
                'prompt_id': prompt_id,
                'prompt_lang': prompt_lang,
                'accuracy': cls_report['accuracy'], 
                'micro_prec': micro_prec,
                'micro_rec': micro_rec,
                'micro_f1_score': micro_f1,
                'macro_prec': cls_report['macro avg']['precision'],
                'macro_rec': cls_report['macro avg']['recall'],
                'macro_f1_score': cls_report['macro avg']['f1-score'],
                'weighted_prec': cls_report['weighted avg']['precision'],
                'weighted_rec': cls_report['weighted avg']['recall'],
                'weighted_f1_score': cls_report['weighted avg']['f1-score'],
            })

    pd.DataFrame(metrics).reset_index().to_csv(f'{metric_dir}/nlu_results_{prompt_lang}_{MODEL.split("/")[-1]}.csv', index=False)
